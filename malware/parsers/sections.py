#!/usr/bin/env python

import argparse
import collections
import json
import logging
import multiprocessing
import os
import re
import sys
import traceback


class SectionParser:
    def __init__(self):
        prefix = r"\.?\w+\:[\dA-F]+\s+;\s+"
        section_regexp = r"\s*\n".join((
            r"\.?(\w+)\:[\dA-F]+\s+;\s+Section\s+[\dA-F]+\.\s+\(virtual\s+address\s+([\dA-F]+)\)".format(prefix),
            r"{0}Virtual\s+size\s+\:\s+([\dA-F]+)\s+\(\s*[\dA-F]+\.\)".format(prefix),
            r"{0}Section\s+size\s+in\s+file\s+\:\s+([\dA-F]+)\s+\(\s*[\dA-F]+\.\)".format(prefix),
            r"{0}Offset\s+to\s+raw\s+data\s+for\s+section\:\s+([\dA-F]+)".format(prefix),
            r"{0}Flags\s+([\dA-F]+)\:\s+([\w\s]+)\n".format(prefix)
        ))
        self.sections_regexp = re.compile(section_regexp)
        self.collapsed_regexp = re.compile(r"\.?(\w+)\:[\dA-F]+\s+;\s*\[\s*([\dA-F]+)\s+BYTES\s*\:\s*COLLAPSED\s+SEGMENT")

    def get_sections_info(self, content):
        section_starts = {}
        for line in content.split("\n"):
            if ":" not in line:
                continue
            p = line.find(":")
            section_name = line[:p]
            if section_name not in section_starts:
                start = int(line[p+1:p+9], 16)
                section_starts[section_name.lstrip(".").lower()] = start

        return {
            "sections": [
                {
                    "name": m[0].lower(),
                    "v_addr": int(m[1], 16),
                    "v_size": int(m[2], 16),
                    "size_in_file": int(m[3], 16),
                    "offset_to_raw_data": int(m[4], 16),
                    "flags": int(m[5], 16),
                    "flags_asc": re.split(r"\s", m[6].lower()),
                    "start": section_starts[m[0].lower()]
                }
                for m in re.findall(self.sections_regexp, content)
            ],
            "collapsed": [
                {
                    "name": m[0].lower(),
                    "size": int(m[1], 16),
                    "start": section_starts[m[0].lower()]
                }
                for m in re.findall(self.collapsed_regexp, content)
            ]
        }


class ImportParser:
    def __init__(self):
        self.imports_regex = re.compile(
            r"\.\w+\:[\dA-F]+\s+;\s+Imports\s+from\s+([\w\.]+)\s*"
        )

        # extrn WideCharToMultiByte:dword
        self.extrn_regex = re.compile(
            r"extrn\s+(\w+)\:\w+"
        )

        self.calls_regex = re.compile(
            r".\w+\:[\dA-F]+\s+([0-9A-F][0-9A-F]\+?\s+)+call\s+(.+)$"
        )

    def get_imports(self, content):
        import_records = []
        calls = collections.defaultdict(int)
        functions = set()
        module = ""
        for line in content.split("\n"):
            m = re.match(self.imports_regex, line)
            if m is not None:
                module = m.group(1)
            else:
                m = re.search(self.extrn_regex, line)
                if m is not None:
                    function = m.group(1)
                    import_records.append("{0}:{1}".format(module, function))
                    functions.add(function.lower().replace("__imp_", ""))

        for line in content.split("\n"):
            m = re.match(self.calls_regex, line)
            if m is not None:
                function = self.__parse_function_call(m.group(2), functions)
                if function in functions:
                    calls[function] += 1

        return import_records, calls

    def __parse_function_call(self, function, imported_functions):
        if ";" in function and function.rfind(";") + 1 < len(function):
            candidate = function[function.rfind(";") + 1:].strip().lower()
            if candidate in imported_functions:
                return candidate

        # strip comment
        if ";" in function:
            function = function[:function.rfind(";")].strip()
        # take last token
        function = re.split(r"\s+", function)[-1]
        for c in ("+", "["):
            if c in function:
                function = function[:function.find(c)]
        function = function.replace("ds:", "")
        return function.lower()


class CodeParser:
    def __init__(self):
        self.code_regexp = re.compile(
            r".\w+\:[\dA-F]+\s+(([0-9A-F][0-9A-F]\+?\s+)+)(.+)$"
        )

        with open("mnemonics.txt", "r") as inp:
            self.mnemonics = set(map(lambda x: x.rstrip(), inp.readlines()))

        self.prefixes = set((
            "rep", "repe", "repz", "repne", "repnz", "lock"
        ))

    def get_instructions(self, content):
        instructions = []
        for line in content.split("\n"):
            m = re.match(self.code_regexp, line)
            if m is not None:
                command = re.split(r"\s+", m.group(3))
                if command[0] in self.prefixes:
                    if len(command) > 1:
                        instructions.append(" ".join((command[0], command[1])))
                    else:
                        instructions.append(command[0])
                else:
                    if command[0] in self.mnemonics:
                        instructions.append(command[0])
        return instructions


def ngram_frequency(items, n):
    freq = collections.defaultdict(int)
    for i in xrange(len(items) - (n - 1)):
        freq[" ".join(items[i:i + n])] += 1
    return freq.items()


class FileParser(multiprocessing.Process):
    def __init__(self, task_queue, result_queue):
        multiprocessing.Process.__init__(self)
        self.task_queue = task_queue
        self.result_queue = result_queue

        self.section_parser = SectionParser()
        self.import_parser = ImportParser()
        self.code_parser = CodeParser()

    def run(self):
        while True:
            next_task = self.task_queue.get()
            if next_task is None:
                self.task_queue.task_done()
                break
            answer = None
            try:
                filename = next_task
                answer = self.parse_file(filename)
            except Exception as err:
                answer = {
                    "filename": filename,
                    "error": 1,
                    "error_str": str(err),
                    "trace": traceback.format_exc()
                }
            self.task_queue.task_done()
            self.result_queue.put(answer)

    def parse_file(self, filename):
        bytes_filename = filename.replace(".asm", ".bytes")

        with open(filename, "r") as inp:
            content = inp.read()

            instructions = self.code_parser.get_instructions(content)
            imports, calls = self.import_parser.get_imports(content)

            info = {
                "filename": filename,
                "asm_size": os.path.getsize(filename),
                "bytes_size": os.path.getsize(bytes_filename),
                "sections": self.section_parser.get_sections_info(content),
                "imports": imports,
                "calls": calls.items(),
                "instructions": {
                    "frequency": collections.Counter(instructions).items(),
                    "2grams": ngram_frequency(instructions, 2),
                    "3grams": ngram_frequency(instructions, 3)
                }
            }

            return info


class ParallelFileParser:
    def __init__(self, num_parsers=None):
        self.num_parsers = multiprocessing.cpu_count() \
            if num_parsers is None else num_parsers

        self.tasks = multiprocessing.JoinableQueue()
        self.results = multiprocessing.Queue()

        self.parsers = [
            FileParser(self.tasks, self.results)
            for i in xrange(self.num_parsers)
        ]

        for parser in self.parsers:
            parser.start()

    def schedule_files(self, filenames):
        for filename in filenames:
            self.tasks.put(filename)
        for _ in xrange(self.num_parsers):
            self.tasks.put(None)

    def join(self):
        self.tasks.join()

    def panic(self):
        for parser in self.parsers:
            parser.terminate()


def chunks(l, n):
    for i in xrange(0, len(l), n):
        yield l[i:i+n]


def main():
    logging.basicConfig(level=logging.INFO)

    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument("data_dir")
    arg_parser.add_argument("output_dir")
    args = arg_parser.parse_args()

    filenames = [
        os.path.join(args.data_dir, filename)
        for filename in os.listdir(args.data_dir)
        if filename.endswith(".asm")
    ]

    part = 0
    for chunk in chunks(filenames, 100):
        parser = ParallelFileParser()
        try:
            logging.info("new chunk")

            parser.schedule_files(chunk)
            parser.join()

            with open(os.path.join(args.output_dir, "parsed_{0}".format(part)), "w") as out:
                for _ in xrange(len(chunk)):
                    answer = parser.results.get()
                    print >>out, json.dumps(answer)

            part += 1
        except Exception:
            parser.panic()
            logging.exception("cannot process chunk")


if __name__ == "__main__":
    sys.exit(main())

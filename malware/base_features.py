#!/usr/bin/env python

import argparse
import collections
import json
import os
import sys

import numpy as np
import scipy as sp

from sklearn import cross_validation
from sklearn import linear_model
from sklearn import metrics

import xgboost as xgb


class BagOfWords:
    def __init__(self):
        self.freq = collections.defaultdict(int)

    def add_word(self, word, count=1):
        self.freq[word] += count


def binary_feature(*args):
    return 1


def tf(df, *args):
    return df


def idf(df, doc_count, n):
    return np.log(float(n) / doc_count)


def tfidf(df, doc_count, n):
    return df * np.log(float(n) / doc_count)


def log_tfidf(df, doc_count, n):
    return (1 + np.log(df)) * np.log(float(n) / doc_count)


# sorry
TRAIN_FEATURE_INDEX = collections.defaultdict(dict)


def make_feature_matrix(bag_id, bags, min_doc_count, feature_function, stage):
    doc_count = collections.defaultdict(int)
    for bag in bags:
        for word in bag.freq.iterkeys():
            doc_count[word] += 1

    feature_index = {}
    if stage == "train":
        for word, freq in doc_count.iteritems():
            if freq >= min_doc_count:
                feature_index[word] = len(feature_index)
        TRAIN_FEATURE_INDEX[bag_id] = feature_index
    else:
        feature_index = TRAIN_FEATURE_INDEX[bag_id]

    object_count = len(bags)
    feature_count = len(feature_index)

    features = np.zeros((object_count, feature_count), dtype=np.float32)
    for object_index, bag in enumerate(bags):
        for word, df in bag.freq.iteritems():
            if word in feature_index:
                features[object_index][feature_index[word]] = \
                    feature_function(df, doc_count[word], object_count)

    return features


class Bags:
    def __init__(self):
        self.bags = collections.defaultdict(list)

    def add_bag(self, bag_id):
        bag = BagOfWords()
        self.bags[bag_id].append(bag)
        return bag

    def get_features(self, bag_id, min_doc_count,
                     stage,
                     feature_function=binary_feature):
        bags = self.bags[bag_id]
        return make_feature_matrix(
                bag_id, bags, min_doc_count, feature_function, stage)


def get_binary_id(info):
    return os.path.splitext(os.path.basename(info["filename"]))[0]


def split_import(import_record):
    module, function = import_record.split(":")
    return module.lower(), import_record.lower().replace("__imp_", "")


class Features:
    def __init__(self, import_features,
                 module_features,
                 code_features,
                 code_bigrams_features,
                 section_features,
                 regular_features):
        # sparse features
        self.import_features = import_features
        self.module_features = module_features
        self.code_features = code_features
        self.code_bigrams_features = code_bigrams_features
        self.section_features = section_features
        self.regular_features = regular_features

    def get_all_features(self):
        all_features = (
            self.import_features,
            self.module_features,
            self.code_features,
            self.code_bigrams_features,
            self.section_features,
            self.regular_features
        )
        return np.concatenate(all_features, axis=1)

    def get_object_count(self):
        return len(self.import_features)


def get_section_size(sections, name):
    for section in sections:
        if name == section["name"].lower():
            return np.log(section["size_in_file"] + 1)
    return 0


def calculate_regular_features(info):
    instruction_count = sum([
        count for _, count in info["instructions"]["frequency"]
    ])

    sections = info["sections"]["sections"]
    collapsed_sections = info["sections"]["collapsed"]

    strange_exec_sections = 0
    exec_sections = 0
    for section in sections:
        if "Executable" in section["flags_asc"]:
            exec_sections += 1
            if section["name"].lower() not in ("code", "text"):
                strange_exec_sections = 1

    reloc_size = 0
    rsrc_size = 0
    for section in collapsed_sections:
        if section["name"].lower() == "rsrc":
            rsrc_size = int(section["size"], 16)
        elif section["name"].lower() == "reloc":
            reloc_size = int(section["size"], 16)

    return [
        np.log(instruction_count + 1) / (exec_sections + 1),
        np.log(len(info["instructions"]["frequency"]) + 1),
        np.log(len(info["imports"]) + 1),
        len(sections),
        len(sections) + len(collapsed_sections),
        len(collapsed_sections),
        get_section_size(sections, "text"),
        get_section_size(sections, "code"),
        get_section_size(sections, "data"),
        get_section_size(sections, "idata"),
        get_section_size(sections, "bss"),
        get_section_size(sections, "tls"),
        exec_sections,
        strange_exec_sections,
        np.log(info["asm_size"] + 1),
        np.log(info["bytes_size"] + 1),
        np.log(rsrc_size + 1),
        np.log(reloc_size + 1)
    ]


def calculate_features(filenames, stage):
    bags = Bags()

    binary_ids = []
    regular_features = []
    for filename in filenames:
        with open(filename, "r") as inp:
            for line in inp:
                info = json.loads(line)
                if info is None:
                    continue

                binary_id = get_binary_id(info)
                binary_ids.append(binary_id)

                import_bag = bags.add_bag("import")
                module_bag = bags.add_bag("module")
                for import_record in info["imports"]:
                    module, function = split_import(import_record)
                    import_bag.add_word(function)
                    module_bag.add_word(module)

                code_bag = bags.add_bag("code")
                for instruction, count in info["instructions"]["frequency"]:
                    code_bag.add_word(instruction, count)

                bigram_code_bag = bags.add_bag("code_bigrams")
                for instruction, count in info["instructions"]["2grams"]:
                    bigram_code_bag.add_word(instruction, count)

                section_bag = bags.add_bag("sections")
                for section in info["sections"]["sections"]:
                    section_bag.add_word(section["name"])
                regular_features.append(calculate_regular_features(info))

    features = Features(
        bags.get_features("import", 5, idf, stage),
        bags.get_features("module", 5, stage),
        bags.get_features("code", 5, stage, log_tfidf),
        bags.get_features("code_bigrams", 50, stage, log_tfidf),
        bags.get_features("sections", 2, stage),
        np.array(regular_features)
    )
    return features, binary_ids


def create_and_apply_model(train_features, train_y, test_features):
    clf = linear_model.LogisticRegression()
    clf.fit(train_features, train_y)
    return clf.predict_proba(test_features)


def do_cv(features, y):
    all_features = features.get_all_features()
    scores = []
    for train_index, test_index in cross_validation.KFold(
            n=features.get_object_count(), n_folds=5, shuffle=True,
            random_state=42):

        x_train, x_test = all_features[train_index], all_features[test_index]
        y_train, y_test = y[train_index], y[test_index]

        xg_train = xgb.DMatrix(x_train, label=y_train)
        xg_test = xgb.DMatrix(x_test, label=y_test)
        xg_params = {
            "silent": 1,
            "objective": "multi:softprob",
            "eta": 0.1,
            "max_depth": 3,
            "nthread": 4,
            "num_class": 9
        }

        watchlist = [(xg_train, "train"), (xg_test, "test")]
        num_round = 100
        bst = xgb.train(xg_params, xg_train, num_round, watchlist)

        predicted = bst.predict(xg_test)
        score = metrics.log_loss(y_test, predicted)
        print score
        scores.append(score)

    print "cv scores:"
    print scores
    mean = np.mean(scores)
    std = np.std(scores)
    print "mean={0}, sd={1}".format(mean, std)
    print "T-confidence interval:"
    print sp.stats.t.interval(
        alpha=0.95, df=len(scores) - 1, loc=mean, scale=std)


def read_target(binary_ids, labels_filename):
    labels = {}
    with open(labels_filename, "r") as inp:
        first_line = True
        for line in inp:
            if first_line:
                first_line = False
            else:
                a, b = line.rstrip().split(",")
                labels[a.strip('"')] = int(b) - 1
    return np.array([
        labels[binary_id] for binary_id in binary_ids
    ])


# todo: the functions train_and_predict and do_cv must be refactores!
def train_and_predict(train_features, train_y, test_features):
    xg_train = xgb.DMatrix(train_features, label=train_y)
    xg_params = {
        "silent": 1,
        "objective": "multi:softprob",
        "eta": 0.1,
        "max_depth": 3,
        "nthread": 4,
        "num_class": 9
    }

    watchlist = [(xg_train, "train")]
    num_round = 100
    bst = xgb.train(xg_params, xg_train, num_round, watchlist)

    xg_test = xgb.DMatrix(test_features)
    return bst.predict(xg_test)


def print_predictions(predictions, binary_ids):
    print ",".join((
        '"Id"',
        '"Prediction1"',
        '"Prediction2"',
        '"Prediction3"',
        '"Prediction4"',
        '"Prediction5"',
        '"Prediction6"',
        '"Prediction7"',
        '"Prediction8"',
        '"Prediction9"'))
    has_prediction = set()
    for binary_id, values in zip(binary_ids, predictions):
        has_prediction.add(binary_id)
        print ",".join(
            ['"{0}"'.format(binary_id)] + map(str, values)
        )

# fix this shit
    with open(r"D:\Kaggle\malware\sampleSubmission.csv") as inp:
        next(inp, None)
        for line in inp:
            binary_id = line.split(",")[0].strip('"')
            if binary_id not in has_prediction:
                print '"{0}",0.14179241810820759,0.22800883327199117,0.27070298122929703,0.043706293706293704,0.0038645564961354434,0.06910195068089806,0.03662127346337873,0.11299227088700772,0.09320942215679058'.format(binary_id)


def main():
    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument("data_dir")
    arg_parser.add_argument("labels")
    arg_parser.add_argument("-s", "--solve", action="store_true")
    arg_parser.add_argument("-t", "--test")
    args = arg_parser.parse_args()

    if args.solve:
        train_filenames = [
            os.path.join(args.data_dir, filename)
            for filename in os.listdir(args.data_dir)
        ]
        train_features, train_binary_ids = \
            calculate_features(train_filenames, "train")
        train_target = read_target(train_binary_ids, args.labels)

        test_filenames = [
            os.path.join(args.test, filename)
            for filename in os.listdir(args.test)
        ]
        test_features, test_binary_ids = \
            calculate_features(test_filenames, "test")

        predictions = train_and_predict(
            train_features.get_all_features(),
            train_target,
            test_features.get_all_features())

        print_predictions(predictions, test_binary_ids)
    else:
        filenames = [
            os.path.join(args.data_dir, filename)
            for filename in os.listdir(args.data_dir)
        ]
        features, binary_ids = calculate_features(filenames, "train")
        target = read_target(binary_ids, args.labels)
        do_cv(features, target)


if __name__ == "__main__":
    sys.exit(main())
